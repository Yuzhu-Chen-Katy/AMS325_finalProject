{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_2genre.csv')\n",
    "df.iloc[:, -1] = df.iloc[:,-1].replace({2:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:-1]\n",
    "X = normalize(X)\n",
    "y = df.iloc[:, -1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0) \n",
    "X_train, X_test, y_train, y_test = X_train.T, X_test.T, y_train.values.reshape(1, 150), y_test.values.reshape(1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "num_iter = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1 / (1 + np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, num_iter=10000, learning_rate=0.5, hidden_layer_num=15):\n",
    "    \n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    w1 = np.random.randn(hidden_layer_num, n) * 0.01 #\n",
    "    b1 = np.random.randn(hidden_layer_num, 1)\n",
    "    w2 = np.random.randn(1, hidden_layer_num) * 0.01 #\n",
    "    b2 = np.random.randn(1, 1)\n",
    "    for i in range(num_iter):\n",
    "        Z1 = np.dot(w1, X_train) + b1\n",
    "        A1 = sigmoid(Z1) \n",
    "        Z2 = np.dot(w2, A1) + b2\n",
    "        A2 = sigmoid(Z2) \n",
    "        cost = np.sum((y_train * np.log(A2)) + ((1-y_train) * np.log(1-A2))) * (-1/m)\n",
    "\n",
    "        dZ2 = A2 - y_train\n",
    "        dW2 = np.dot(dZ2, A1.T) / m  #######\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True) * (1 / m)\n",
    "\n",
    "        dA1 = np.dot(w2.T, dZ2)\n",
    "        dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n",
    "        dW1 = np.dot(dZ1, X_train.T) / m #######\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True) * (1 / m)\n",
    "\n",
    "        w2 = w2 - learning_rate * dW2\n",
    "        b2 = b2 - learning_rate * db2\n",
    "        w1 = w1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1\n",
    "\n",
    "        if(i %1000 == 0):\n",
    "            print(f'At epoch {i}, cost is {cost}')\n",
    "    return [w1, w2, b1, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0, cost is 0.6983904061039693\n",
      "At epoch 1000, cost is 0.15380232514844563\n",
      "At epoch 2000, cost is 0.07617707116453971\n",
      "At epoch 3000, cost is 0.058306355223167676\n",
      "At epoch 4000, cost is 0.05098309807245595\n",
      "At epoch 5000, cost is 0.047083470152077644\n",
      "At epoch 6000, cost is 0.04468171702157733\n",
      "At epoch 7000, cost is 0.04304304330866654\n",
      "At epoch 8000, cost is 0.041835867344083344\n",
      "At epoch 9000, cost is 0.04089230873007523\n",
      "At epoch 10000, cost is 0.04011943129680138\n",
      "At epoch 11000, cost is 0.039462229836810206\n",
      "At epoch 12000, cost is 0.03888647812064723\n",
      "At epoch 13000, cost is 0.03837003595225454\n",
      "At epoch 14000, cost is 0.03789811966760169\n",
      "At epoch 15000, cost is 0.03746058335594751\n",
      "At epoch 16000, cost is 0.03705028752401399\n",
      "At epoch 17000, cost is 0.036662086793999175\n",
      "At epoch 18000, cost is 0.036292184800545496\n",
      "At epoch 19000, cost is 0.03593771423942997\n"
     ]
    }
   ],
   "source": [
    "params = train(X_train, y_train,num_iter=20000, learning_rate=1, hidden_layer_num=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.1155\n",
      "Accuracy: 98.67%\n",
      "RMSE: 0.1414\n",
      "Accuracy: 98.00%\n"
     ]
    }
   ],
   "source": [
    "def score(X, y, w1, w2, b1, b2):\n",
    "    Z1 = np.dot(w1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(w2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    # Convert predicted probabilities to binary predictions (0 or 1)\n",
    "    Y_prediction = (A2 > 0.5).astype(float)\n",
    "    \n",
    "    # Compute RMSE\n",
    "    rmse = np.sqrt(np.mean((Y_prediction - y)**2))\n",
    "    \n",
    "    print(\"RMSE: {:.4f}\".format(rmse))\n",
    "    \n",
    "    # Optionally, you can print other classification metrics\n",
    "    accuracy = 100 - np.mean(np.abs(Y_prediction - y)) * 100\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy))\n",
    "\n",
    "# Evaluate on the training set\n",
    "score(X_train, y_train, *params)\n",
    "score(X_test, y_test, *params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 92.67%\n",
      "Testing Accuracy: 96.00%\n"
     ]
    }
   ],
   "source": [
    "#Logistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Read data and preprocess\n",
    "df = pd.read_csv('data_2genre.csv')\n",
    "df.iloc[:, -1] = df.iloc[:, -1].replace({2: 0})\n",
    "X = df.iloc[:, 1:-1]\n",
    "X = normalize(X)\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Specify the test size as 25%\n",
    "test_size = 0.25\n",
    "\n",
    "# Split data into training and testing sets with the specified test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training and testing sets\n",
    "y_train_pred = logreg_model.predict(X_train)\n",
    "y_test_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print accuracies\n",
    "print(\"Training Accuracy: {:.2f}%\".format(train_accuracy * 100))\n",
    "print(\"Testing Accuracy: {:.2f}%\".format(test_accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation Scores: [0.9   0.95  0.925 0.975 0.925]\n",
      "Mean CV Accuracy: 93.50%\n",
      "Cross-validation RMSE Scores: [0.10837562 0.17920631 0.14065937 0.20159851 0.21436923]\n",
      "Mean RMSE: 0.17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Read data and preprocess\n",
    "df = pd.read_csv('data_2genre.csv')\n",
    "df.iloc[:, -1] = df.iloc[:, -1].replace({2: 0})\n",
    "X = df.iloc[:, 1:-1]\n",
    "X = normalize(X)\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Initialize Linear Regression model with cross-validation\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_predictions = cross_val_score(linear_model, X, y, cv=5, scoring='neg_mean_squared_error')  # Use neg_mean_squared_error for RMSE\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_scores = np.sqrt(-cv_predictions)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(logreg_model, X, y, cv=5)  # adjust the number of folds (cv)\n",
    "print(\"Cross-validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy: {:.2f}%\".format(np.mean(cv_scores) * 100))\n",
    "\n",
    "\n",
    "# Print RMSE scores and mean RMSE\n",
    "print(\"Cross-validation RMSE Scores:\", rmse_scores)\n",
    "print(\"Mean RMSE: {:.2f}\".format(np.mean(rmse_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/katyc/Downloads/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katyc\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\katyc\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, 1:-1]\n",
    "X = normalize(X)\n",
    "\n",
    "y = data.iloc[:, -1] \n",
    "y_labelled = LabelEncoder().fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_labelled, random_state = 0) \n",
    "X_train, X_test, y_train, y_test = X_train.T, X_test.T, y_train.reshape(1, 750), y_test.reshape(1, 250)\n",
    "\n",
    "\n",
    "yone_train = OneHotEncoder(sparse=False).fit_transform(y_train.reshape(-1, 1), y=None)\n",
    "yone_train = yone_train.T\n",
    "\n",
    "yone_test = OneHotEncoder(sparse=False).fit_transform(y_test.reshape(-1, 1), y=None)\n",
    "yone_test = yone_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi(X_train, y_train, num_iter=10000, learning_rate=0.5, hidden_layer_num=15):\n",
    "    \n",
    "    n = X_train.shape[0]\n",
    "    m = y_train.shape[1]\n",
    "    w1 = np.random.randn(hidden_layer_num, n) * 0.01 #\n",
    "    b1 = np.random.randn(hidden_layer_num, 1)\n",
    "    w2 = np.random.randn(10, hidden_layer_num) * 0.01 #\n",
    "    b2 = np.random.randn(10, 1)\n",
    "    for i in range(num_iter):\n",
    "        Z1 = np.dot(w1, X_train) + b1\n",
    "        A1 = sigmoid(Z1) \n",
    "        Z2 = np.dot(w2, A1) + b2\n",
    "        A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0) # SOFTMAX\n",
    "        cost = np.sum(y_train * np.log(A2)) * (-1/m)\n",
    "\n",
    "        dZ2 = A2 - y_train\n",
    "        dW2 = np.dot(dZ2, A1.T) / m  #######\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True) * (1 / m)\n",
    "\n",
    "        dA1 = np.dot(w2.T, dZ2)\n",
    "        dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n",
    "        dW1 = np.dot(dZ1, X_train.T) / m #######\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True) * (1 / m)\n",
    "\n",
    "        w2 = w2 - learning_rate * dW2\n",
    "        b2 = b2 - learning_rate * db2\n",
    "        w1 = w1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1\n",
    "\n",
    "        if(i %1000 == 0):\n",
    "            print(f'At epoch {i}, cost is {cost}')\n",
    "    return [w1, w2, b1, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_multi(X, y, w1, w2, b1, b2):\n",
    "    # y is one-hot-encoded\n",
    "    Z1 = np.dot(w1, X) + b1\n",
    "    A1 = sigmoid(Z1) \n",
    "    Z2 = np.dot(w2, A1) + b2\n",
    "    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0) # SOFTMAX\n",
    "    #I changed 750 to 250\n",
    "    Y_predictions = np.argmax(A2, axis=0).reshape(250,1)\n",
    "    labels = np.argmax(y, axis=0)\n",
    "    print(classification_report(Y_predictions, labels))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0, cost is 2.9123970666191936\n",
      "At epoch 1000, cost is 2.299150528194198\n",
      "At epoch 2000, cost is 2.2624535858579606\n",
      "At epoch 3000, cost is 2.067579057677499\n",
      "At epoch 4000, cost is 1.98066823854925\n",
      "At epoch 5000, cost is 1.9520329847344613\n",
      "At epoch 6000, cost is 1.9371226828942951\n",
      "At epoch 7000, cost is 1.9248022793468809\n",
      "At epoch 8000, cost is 1.909937134230339\n",
      "At epoch 9000, cost is 1.8863354662226228\n",
      "At epoch 10000, cost is 1.849141818319388\n",
      "At epoch 11000, cost is 1.8087599109770076\n",
      "At epoch 12000, cost is 1.780790627471398\n",
      "At epoch 13000, cost is 1.7647781758077847\n",
      "At epoch 14000, cost is 1.7551395713603832\n",
      "At epoch 15000, cost is 1.7484316606168613\n",
      "At epoch 16000, cost is 1.7430732283139017\n",
      "At epoch 17000, cost is 1.7383712172985475\n",
      "At epoch 18000, cost is 1.7340089573329223\n",
      "At epoch 19000, cost is 1.7298879668061395\n",
      "At epoch 20000, cost is 1.7378044649182587\n",
      "At epoch 21000, cost is 1.734022911520261\n",
      "At epoch 22000, cost is 1.7306847731586281\n",
      "At epoch 23000, cost is 1.7272644735164056\n",
      "At epoch 24000, cost is 1.723663916603194\n",
      "At epoch 25000, cost is 1.7197992615571076\n",
      "At epoch 26000, cost is 1.715584473991894\n",
      "At epoch 27000, cost is 1.7109360767165356\n",
      "At epoch 28000, cost is 1.7057853494528026\n",
      "At epoch 29000, cost is 1.70009777693229\n",
      "At epoch 30000, cost is 1.6938985955192953\n",
      "At epoch 31000, cost is 1.6872990706984097\n",
      "At epoch 32000, cost is 1.680515749820957\n",
      "At epoch 33000, cost is 1.673874650856883\n",
      "At epoch 34000, cost is 1.6677714040628688\n",
      "At epoch 35000, cost is 1.6624798380591561\n",
      "At epoch 36000, cost is 1.657780846739898\n",
      "At epoch 37000, cost is 1.6530753318981393\n",
      "At epoch 38000, cost is 1.648102842735993\n",
      "At epoch 39000, cost is 1.642947800410907\n",
      "At epoch 40000, cost is 1.6377078948891761\n",
      "At epoch 41000, cost is 1.6324279348163024\n",
      "At epoch 42000, cost is 1.627125635630263\n",
      "At epoch 43000, cost is 1.6218174240695662\n",
      "At epoch 44000, cost is 1.6165371475613057\n",
      "At epoch 45000, cost is 1.6113320997589569\n",
      "At epoch 46000, cost is 1.6062303917952483\n",
      "At epoch 47000, cost is 1.6012209717928265\n",
      "At epoch 48000, cost is 1.5962727378996666\n",
      "At epoch 49000, cost is 1.5913601706427725\n",
      "At epoch 50000, cost is 1.5864709592507884\n",
      "At epoch 51000, cost is 1.5816020527467392\n",
      "At epoch 52000, cost is 1.5767543379825875\n",
      "At epoch 53000, cost is 1.57152117018266\n",
      "At epoch 54000, cost is 1.57731884806482\n",
      "At epoch 55000, cost is 1.559965685207344\n",
      "At epoch 56000, cost is 1.5563404766832378\n",
      "At epoch 57000, cost is 1.5530403134396589\n",
      "At epoch 58000, cost is 1.5498461800679542\n",
      "At epoch 59000, cost is 1.5458848504404958\n",
      "At epoch 60000, cost is 1.5452338175572857\n",
      "At epoch 61000, cost is 1.5408086048382166\n",
      "At epoch 62000, cost is 1.5356065737072928\n",
      "At epoch 63000, cost is 1.5335774511165137\n",
      "At epoch 64000, cost is 1.5292567281394975\n",
      "At epoch 65000, cost is 1.5440113416784838\n",
      "At epoch 66000, cost is 1.5215017960735562\n",
      "At epoch 67000, cost is 1.5183422170541798\n",
      "At epoch 68000, cost is 1.5152666116558993\n",
      "At epoch 69000, cost is 1.5106027774187594\n",
      "At epoch 70000, cost is 1.5074772134234105\n",
      "At epoch 71000, cost is 1.50415457943239\n",
      "At epoch 72000, cost is 1.4974058469108882\n",
      "At epoch 73000, cost is 1.5055174417858082\n",
      "At epoch 74000, cost is 1.4919334349909743\n",
      "At epoch 75000, cost is 1.4912456673284735\n",
      "At epoch 76000, cost is 1.4856530252862288\n",
      "At epoch 77000, cost is 1.4943488858955019\n",
      "At epoch 78000, cost is 1.4815993262614802\n",
      "At epoch 79000, cost is 1.4841369569083929\n",
      "At epoch 80000, cost is 1.4774493730696474\n",
      "At epoch 81000, cost is 1.4782940402394573\n",
      "At epoch 82000, cost is 1.4681413794162406\n",
      "At epoch 83000, cost is 1.4586593449562133\n",
      "At epoch 84000, cost is 1.4653809042216737\n",
      "At epoch 85000, cost is 1.4682572909662939\n",
      "At epoch 86000, cost is 1.4610516961406126\n",
      "At epoch 87000, cost is 1.458761640748145\n",
      "At epoch 88000, cost is 1.429957699924976\n",
      "At epoch 89000, cost is 1.4510131494290632\n",
      "At epoch 90000, cost is 1.43561492005464\n",
      "At epoch 91000, cost is 1.5158734369006461\n",
      "At epoch 92000, cost is 1.4448956118750222\n",
      "At epoch 93000, cost is 1.4382902602816212\n",
      "At epoch 94000, cost is 1.4307920834773915\n",
      "At epoch 95000, cost is 1.4391203388168432\n",
      "At epoch 96000, cost is 1.4532426044948357\n",
      "At epoch 97000, cost is 1.4388205259631577\n",
      "At epoch 98000, cost is 1.426963346352566\n",
      "At epoch 99000, cost is 1.4281415063254799\n",
      "At epoch 100000, cost is 1.4282655190115523\n",
      "At epoch 101000, cost is 1.430325190955849\n",
      "At epoch 102000, cost is 1.4293505033618203\n",
      "At epoch 103000, cost is 1.4404297324894162\n",
      "At epoch 104000, cost is 1.4445694023406703\n",
      "At epoch 105000, cost is 1.4458702043569471\n",
      "At epoch 106000, cost is 1.4266275219201716\n",
      "At epoch 107000, cost is 1.4413490527917285\n",
      "At epoch 108000, cost is 1.4080925150869452\n",
      "At epoch 109000, cost is 1.437445770537833\n",
      "At epoch 110000, cost is 1.407535144969887\n",
      "At epoch 111000, cost is 1.449475263478409\n",
      "At epoch 112000, cost is 1.3992938442592233\n",
      "At epoch 113000, cost is 1.4041044036220967\n",
      "At epoch 114000, cost is 1.412569077829692\n",
      "At epoch 115000, cost is 1.4055060697504407\n",
      "At epoch 116000, cost is 1.3883435146702752\n",
      "At epoch 117000, cost is 1.399343932819542\n",
      "At epoch 118000, cost is 1.403076418424631\n",
      "At epoch 119000, cost is 1.3967330981529116\n",
      "At epoch 120000, cost is 1.3956434797982387\n",
      "At epoch 121000, cost is 1.3762254175161113\n",
      "At epoch 122000, cost is 1.4071620162798186\n",
      "At epoch 123000, cost is 1.3782983722841426\n",
      "At epoch 124000, cost is 1.4193079882464623\n",
      "At epoch 125000, cost is 1.3807566645978768\n",
      "At epoch 126000, cost is 1.3973793198201891\n",
      "At epoch 127000, cost is 1.364100164109042\n",
      "At epoch 128000, cost is 1.3932923842250826\n",
      "At epoch 129000, cost is 1.3626898045281717\n",
      "At epoch 130000, cost is 1.3830636920935209\n",
      "At epoch 131000, cost is 1.3956783457649908\n",
      "At epoch 132000, cost is 1.3757198409000841\n",
      "At epoch 133000, cost is 1.4149051195131233\n",
      "At epoch 134000, cost is 1.3673208528112357\n",
      "At epoch 135000, cost is 1.4062247350057535\n",
      "At epoch 136000, cost is 1.3666146594396196\n",
      "At epoch 137000, cost is 1.4028531105282347\n",
      "At epoch 138000, cost is 1.3654580639279816\n",
      "At epoch 139000, cost is 1.3453164756367106\n",
      "At epoch 140000, cost is 1.374390938773908\n",
      "At epoch 141000, cost is 1.360234968013229\n",
      "At epoch 142000, cost is 1.3562509975968302\n",
      "At epoch 143000, cost is 1.3469040287697358\n",
      "At epoch 144000, cost is 1.3810855162794147\n",
      "At epoch 145000, cost is 1.363689614980023\n",
      "At epoch 146000, cost is 1.3900681904126697\n",
      "At epoch 147000, cost is 1.3889579717227536\n",
      "At epoch 148000, cost is 1.3490186437284013\n",
      "At epoch 149000, cost is 1.3365983332806544\n",
      "At epoch 150000, cost is 1.3362304120809785\n",
      "At epoch 151000, cost is 1.3605379926261023\n",
      "At epoch 152000, cost is 1.340198797536688\n",
      "At epoch 153000, cost is 1.3531385430174359\n",
      "At epoch 154000, cost is 1.3456541476607342\n",
      "At epoch 155000, cost is 1.337579291806215\n",
      "At epoch 156000, cost is 1.3380126990936245\n",
      "At epoch 157000, cost is 1.3406270012828378\n",
      "At epoch 158000, cost is 1.3340028507460167\n",
      "At epoch 159000, cost is 1.3387568871639264\n",
      "At epoch 160000, cost is 1.3352144802251236\n",
      "At epoch 161000, cost is 1.3299326098349178\n",
      "At epoch 162000, cost is 1.361378678830817\n",
      "At epoch 163000, cost is 1.3298255018317648\n",
      "At epoch 164000, cost is 1.337278187955621\n",
      "At epoch 165000, cost is 1.3195769125775731\n",
      "At epoch 166000, cost is 1.316269364959551\n",
      "At epoch 167000, cost is 1.3250497747041714\n",
      "At epoch 168000, cost is 1.3200625439321183\n",
      "At epoch 169000, cost is 1.3220936655006834\n",
      "At epoch 170000, cost is 1.336297701034851\n",
      "At epoch 171000, cost is 1.3521907749315294\n",
      "At epoch 172000, cost is 1.3251983359800075\n",
      "At epoch 173000, cost is 1.306945277505914\n",
      "At epoch 174000, cost is 1.3166721174828016\n",
      "At epoch 175000, cost is 1.30897460040066\n",
      "At epoch 176000, cost is 1.300093710139447\n",
      "At epoch 177000, cost is 1.3118794851959663\n",
      "At epoch 178000, cost is 1.3229237100236104\n",
      "At epoch 179000, cost is 1.3175262529338876\n",
      "At epoch 180000, cost is 1.2991221191708013\n",
      "At epoch 181000, cost is 1.3058334168683983\n",
      "At epoch 182000, cost is 1.3104728467698967\n",
      "At epoch 183000, cost is 1.3046373547556815\n",
      "At epoch 184000, cost is 1.3043588199842375\n",
      "At epoch 185000, cost is 1.363380897570448\n",
      "At epoch 186000, cost is 1.2862131612355658\n",
      "At epoch 187000, cost is 1.403335479929588\n",
      "At epoch 188000, cost is 1.291442587333936\n",
      "At epoch 189000, cost is 1.3068482044255503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 190000, cost is 1.304122511509458\n",
      "At epoch 191000, cost is 1.2982116861845436\n",
      "At epoch 192000, cost is 1.3071358559739785\n",
      "At epoch 193000, cost is 1.3410476883144031\n",
      "At epoch 194000, cost is 1.2844764137923648\n",
      "At epoch 195000, cost is 1.2911986456814366\n",
      "At epoch 196000, cost is 1.288914030377447\n",
      "At epoch 197000, cost is 1.2916614704730693\n",
      "At epoch 198000, cost is 1.3007074619613657\n",
      "At epoch 199000, cost is 1.330189866664336\n"
     ]
    }
   ],
   "source": [
    "params = train_multi(X_train, yone_train,num_iter=200000, learning_rate=0.5, hidden_layer_num=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.55      0.54        20\n",
      "           1       0.88      0.70      0.78        20\n",
      "           2       0.23      0.58      0.33        12\n",
      "           3       0.42      0.44      0.43        25\n",
      "           4       0.14      0.40      0.21        10\n",
      "           5       0.15      0.67      0.25         6\n",
      "           6       0.89      0.63      0.74        38\n",
      "           7       0.81      0.43      0.56        49\n",
      "           8       0.60      0.19      0.29        63\n",
      "           9       0.03      0.14      0.05         7\n",
      "\n",
      "    accuracy                           0.44       250\n",
      "   macro avg       0.47      0.47      0.42       250\n",
      "weighted avg       0.62      0.44      0.48       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_multi(X_test, yone_test, *params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.57      0.45        51\n",
      "           1       0.95      0.84      0.89        95\n",
      "           2       0.14      0.56      0.23        18\n",
      "           3       0.43      0.47      0.45        68\n",
      "           4       0.35      0.37      0.36        67\n",
      "           5       0.19      0.70      0.30        20\n",
      "           6       0.86      0.55      0.67       114\n",
      "           7       0.85      0.54      0.66       116\n",
      "           8       0.80      0.36      0.50       177\n",
      "           9       0.11      0.33      0.17        24\n",
      "\n",
      "    accuracy                           0.52       750\n",
      "   macro avg       0.51      0.53      0.47       750\n",
      "weighted avg       0.68      0.52      0.56       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_multi(X_train, yone_train, *params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
